{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e9362d-a08c-4c07-9770-c5984587e3dd",
   "metadata": {},
   "source": [
    "## Compute image feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88ba92ec-0c3a-4582-a853-503aaca171d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, CLIPTextModel\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "from  tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# from transformers import CLIPFeatureExtractor, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb3d8d-9946-473c-a252-4608484f787b",
   "metadata": {},
   "source": [
    "### All folders recursively will be checked in this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51d1f5b-c3ff-4e7b-9add-7b65c9dde844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path('images/personal')\n",
    "path = Path('images/philippines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ab8559-636a-43a5-a5e6-bbc990df912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128  # batch size although does not seem to differ too much\n",
    "model_id = \"openai/clip-vit-base-patch32\"   # preconfigured with image size = 224: https://huggingface.co/openai/clip-vit-base-patch32/blob/main/preprocessor_config.json\n",
    "# model_id = \"openai/clip-vit-large-patch14-336\"  # preconfigured with image size = 336: https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/preprocessor_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590da9f8-ac61-43d4-aae6-fabe76774774",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb50f3-f173-44a5-af3c-25ec16bdf520",
   "metadata": {},
   "source": [
    "### Compute the embeddings of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf554ef-319a-48f6-892a-fb0bb7f7638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_feats(images):\n",
    "    input_images = processor(text=None, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output_images_features = model.get_image_features(**input_images).detach()  # don't keep grad data and avoid run out of memory\n",
    "    images_embeds = output_images_features / output_images_features.norm(p=2, dim=-1, keepdim=True)  # normalized features\n",
    "    return images_embeds.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f03a3f-da02-46fd-946a-691b108f8324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_feats(image):\n",
    "    input_image = processor(text=None, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    output_image_features = model.get_image_features(**input_image).detach()\n",
    "    image_embeds = output_image_features / output_image_features.norm(p=2, dim=-1, keepdim=True)  # normalized features\n",
    "    return image_embeds.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f754b27a-093f-4150-a675-36e668264295",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2335it [03:26, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Blue-backed_parrot-68.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5430it [08:01, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Pagong_-177.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9963it [14:34, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Crocodylus_mindorensis-166.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11482it [16:55, 19.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Southeast_Asian_box_turtle-234.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21173it [30:57, 16.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Philippine_crocodile-183.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24305it [35:27, 16.59it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (104458356 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "24974it [36:25, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Southeast_Asian_box_turtle-158.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25487it [37:08, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Southeast_Asian_box_turtle-58.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25536it [37:12, 15.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Southeast_Asian_box_turtle-302.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28318it [41:16, 10.19it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:822: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "28745it [41:54, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Malayan_box_turtle-261.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29231it [42:41, 11.59it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (95376000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "29582it [43:15, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Mago_-60.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31742it [46:24, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Binturong-23.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34409it [50:15,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Green_turtle-453.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34949it [51:06, 14.64it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:822: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "37014it [54:00, 10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Mago_-180.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37703it [54:59, 13.66it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "37964it [55:22, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Blue-naped_parrot-242.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41032it [59:32, 11.80it/s]/home/hwasiti/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (102485149 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "42142it [1:01:08, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Manis_culionensis-2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46849it [1:08:02, 17.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open images/philippines/images_world/flickr_Southeast_Asian_box_turtle-161.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56988it [1:22:28, 11.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# For BS = 1 only [For testing only]\n",
    "feats=[]\n",
    "paths = []\n",
    "for fn in tqdm(path.rglob('*.*')):\n",
    "    try:\n",
    "        image = Image.open(fn)\n",
    "    except:\n",
    "        print(f'Failed to open {fn}')\n",
    "        continue\n",
    "    paths.append(fn)\n",
    "    feats.append(get_image_feats(image))\n",
    "\n",
    "df = pd.DataFrame(zip(paths, feats), columns=['path', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b52140-263d-4993-b7c9-25bb40abb281",
   "metadata": {},
   "source": [
    "### The same but with BS batch size [in my test the same speed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99578009-4d22-4705-9403-cf41708a0a51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "511it [00:34, 14.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m BS:\n\u001b[0;32m---> 15\u001b[0m     feats\u001b[38;5;241m.\u001b[39mextend(\u001b[43mget_image_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mget_image_feats\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image_feats\u001b[39m(image):\n\u001b[0;32m----> 2\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     output_image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_image_features(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_image)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      4\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m output_image_features \u001b[38;5;241m/\u001b[39m output_image_features\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# normalized features\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/transformers/models/clip/processing_clip.py:85\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:150\u001b[0m, in \u001b[0;36mCLIPFeatureExtractor.__call__\u001b[0;34m(self, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# transformations (convert rgb + resizing + center cropping + normalization)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_convert_rgb:\n\u001b[0;32m--> 150\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_resize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample, default_to_square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    155\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:150\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# transformations (convert rgb + resizing + center cropping + normalization)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_convert_rgb:\n\u001b[0;32m--> 150\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_resize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample, default_to_square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    155\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/transformers/image_utils.py:131\u001b[0m, in \u001b[0;36mImageFeatureExtractionMixin.convert_rgb\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fastai/lib/python3.8/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# feats=[]\n",
    "# paths = []\n",
    "# images = []\n",
    "# idx = 0\n",
    "# for fn in tqdm(path.rglob('*.*')):\n",
    "#     try:\n",
    "#         image = Image.open(fn)\n",
    "#     except:\n",
    "#         print(f'Failed to open {fn}')\n",
    "#         continue\n",
    "#     images.append(image)\n",
    "#     paths.append(fn)\n",
    "#     idx += 1\n",
    "#     if idx == BS:\n",
    "#         feats.extend(get_image_feats(images))\n",
    "#         images = []\n",
    "#         idx = 0\n",
    "# if len(images)>0:\n",
    "#         feats.extend(get_image_feats(images))\n",
    "#         images = []\n",
    "# df = pd.DataFrame(zip(paths, feats), columns=['path', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b258d5-3a7a-4eab-aee1-a5baa36b3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(str(path).replace('/', '-')+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1c0f2-af82-4fb7-a3ec-7a229b3f357e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
